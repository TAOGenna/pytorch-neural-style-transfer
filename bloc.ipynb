{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision.models import resnet50\n",
    "from torchvision.models.feature_extraction import get_graph_node_names\n",
    "from torchvision.models.feature_extraction import create_feature_extractor\n",
    "from torchvision.models.detection.mask_rcnn import MaskRCNN\n",
    "from torchvision.models.detection.backbone_utils import LastLevelMaxPool\n",
    "from torchvision.ops.feature_pyramid_network import FeaturePyramidNetwork\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['x', 'conv1', 'bn1', 'relu', 'maxpool', 'layer1.0.conv1', 'layer1.0.bn1', 'layer1.0.relu', 'layer1.0.conv2', 'layer1.0.bn2', 'layer1.0.add', 'layer1.0.relu_1', 'layer1.1.conv1', 'layer1.1.bn1', 'layer1.1.relu', 'layer1.1.conv2', 'layer1.1.bn2', 'layer1.1.add', 'layer1.1.relu_1', 'layer2.0.conv1', 'layer2.0.bn1', 'layer2.0.relu', 'layer2.0.conv2', 'layer2.0.bn2', 'layer2.0.downsample.0', 'layer2.0.downsample.1', 'layer2.0.add', 'layer2.0.relu_1', 'layer2.1.conv1', 'layer2.1.bn1', 'layer2.1.relu', 'layer2.1.conv2', 'layer2.1.bn2', 'layer2.1.add', 'layer2.1.relu_1', 'layer3.0.conv1', 'layer3.0.bn1', 'layer3.0.relu', 'layer3.0.conv2', 'layer3.0.bn2', 'layer3.0.downsample.0', 'layer3.0.downsample.1', 'layer3.0.add', 'layer3.0.relu_1', 'layer3.1.conv1', 'layer3.1.bn1', 'layer3.1.relu', 'layer3.1.conv2', 'layer3.1.bn2', 'layer3.1.add', 'layer3.1.relu_1', 'layer4.0.conv1', 'layer4.0.bn1', 'layer4.0.relu', 'layer4.0.conv2', 'layer4.0.bn2', 'layer4.0.downsample.0', 'layer4.0.downsample.1', 'layer4.0.add', 'layer4.0.relu_1', 'layer4.1.conv1', 'layer4.1.bn1', 'layer4.1.relu', 'layer4.1.conv2', 'layer4.1.bn2', 'layer4.1.add', 'layer4.1.relu_1', 'avgpool', 'flatten', 'fc']\n",
      "ResNet(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Module(\n",
      "    (0): Module(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): Module(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Module(\n",
      "    (0): Module(\n",
      "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Module(\n",
      "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Module(\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Module(\n",
      "    (0): Module(\n",
      "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Module(\n",
      "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Module(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "\n",
      "\n",
      "def forward(self, x : torch.Tensor):\n",
      "    conv1 = self.conv1(x);  x = None\n",
      "    bn1 = self.bn1(conv1);  conv1 = None\n",
      "    relu = self.relu(bn1);  bn1 = None\n",
      "    maxpool = self.maxpool(relu);  relu = None\n",
      "    layer1_0_conv1 = getattr(self.layer1, \"0\").conv1(maxpool)\n",
      "    layer1_0_bn1 = getattr(self.layer1, \"0\").bn1(layer1_0_conv1);  layer1_0_conv1 = None\n",
      "    layer1_0_relu = getattr(self.layer1, \"0\").relu(layer1_0_bn1);  layer1_0_bn1 = None\n",
      "    layer1_0_conv2 = getattr(self.layer1, \"0\").conv2(layer1_0_relu);  layer1_0_relu = None\n",
      "    layer1_0_bn2 = getattr(self.layer1, \"0\").bn2(layer1_0_conv2);  layer1_0_conv2 = None\n",
      "    add = layer1_0_bn2 + maxpool;  layer1_0_bn2 = maxpool = None\n",
      "    layer1_0_relu_1 = getattr(self.layer1, \"0\").relu(add);  add = None\n",
      "    layer1_1_conv1 = getattr(self.layer1, \"1\").conv1(layer1_0_relu_1)\n",
      "    layer1_1_bn1 = getattr(self.layer1, \"1\").bn1(layer1_1_conv1);  layer1_1_conv1 = None\n",
      "    layer1_1_relu = getattr(self.layer1, \"1\").relu(layer1_1_bn1);  layer1_1_bn1 = None\n",
      "    layer1_1_conv2 = getattr(self.layer1, \"1\").conv2(layer1_1_relu);  layer1_1_relu = None\n",
      "    layer1_1_bn2 = getattr(self.layer1, \"1\").bn2(layer1_1_conv2);  layer1_1_conv2 = None\n",
      "    add_1 = layer1_1_bn2 + layer1_0_relu_1;  layer1_1_bn2 = layer1_0_relu_1 = None\n",
      "    layer1_1_relu_1 = getattr(self.layer1, \"1\").relu(add_1);  add_1 = None\n",
      "    layer2_0_conv1 = getattr(self.layer2, \"0\").conv1(layer1_1_relu_1)\n",
      "    layer2_0_bn1 = getattr(self.layer2, \"0\").bn1(layer2_0_conv1);  layer2_0_conv1 = None\n",
      "    layer2_0_relu = getattr(self.layer2, \"0\").relu(layer2_0_bn1);  layer2_0_bn1 = None\n",
      "    layer2_0_conv2 = getattr(self.layer2, \"0\").conv2(layer2_0_relu);  layer2_0_relu = None\n",
      "    layer2_0_bn2 = getattr(self.layer2, \"0\").bn2(layer2_0_conv2);  layer2_0_conv2 = None\n",
      "    layer2_0_downsample_0 = getattr(getattr(self.layer2, \"0\").downsample, \"0\")(layer1_1_relu_1)\n",
      "    layer2_0_downsample_1 = getattr(getattr(self.layer2, \"0\").downsample, \"1\")(layer2_0_downsample_0);  layer2_0_downsample_0 = None\n",
      "    add_2 = layer2_0_bn2 + layer2_0_downsample_1;  layer2_0_bn2 = layer2_0_downsample_1 = None\n",
      "    layer2_0_relu_1 = getattr(self.layer2, \"0\").relu(add_2);  add_2 = None\n",
      "    layer2_1_conv1 = getattr(self.layer2, \"1\").conv1(layer2_0_relu_1)\n",
      "    layer2_1_bn1 = getattr(self.layer2, \"1\").bn1(layer2_1_conv1);  layer2_1_conv1 = None\n",
      "    layer2_1_relu = getattr(self.layer2, \"1\").relu(layer2_1_bn1);  layer2_1_bn1 = None\n",
      "    layer2_1_conv2 = getattr(self.layer2, \"1\").conv2(layer2_1_relu);  layer2_1_relu = None\n",
      "    layer2_1_bn2 = getattr(self.layer2, \"1\").bn2(layer2_1_conv2);  layer2_1_conv2 = None\n",
      "    add_3 = layer2_1_bn2 + layer2_0_relu_1;  layer2_1_bn2 = layer2_0_relu_1 = None\n",
      "    layer2_1_relu_1 = getattr(self.layer2, \"1\").relu(add_3);  add_3 = None\n",
      "    layer3_0_conv1 = getattr(self.layer3, \"0\").conv1(layer2_1_relu_1)\n",
      "    layer3_0_bn1 = getattr(self.layer3, \"0\").bn1(layer3_0_conv1);  layer3_0_conv1 = None\n",
      "    layer3_0_relu = getattr(self.layer3, \"0\").relu(layer3_0_bn1);  layer3_0_bn1 = None\n",
      "    layer3_0_conv2 = getattr(self.layer3, \"0\").conv2(layer3_0_relu);  layer3_0_relu = None\n",
      "    layer3_0_bn2 = getattr(self.layer3, \"0\").bn2(layer3_0_conv2);  layer3_0_conv2 = None\n",
      "    layer3_0_downsample_0 = getattr(getattr(self.layer3, \"0\").downsample, \"0\")(layer2_1_relu_1);  layer2_1_relu_1 = None\n",
      "    layer3_0_downsample_1 = getattr(getattr(self.layer3, \"0\").downsample, \"1\")(layer3_0_downsample_0);  layer3_0_downsample_0 = None\n",
      "    add_4 = layer3_0_bn2 + layer3_0_downsample_1;  layer3_0_bn2 = layer3_0_downsample_1 = None\n",
      "    layer3_0_relu_1 = getattr(self.layer3, \"0\").relu(add_4);  add_4 = None\n",
      "    layer3_1_conv1 = getattr(self.layer3, \"1\").conv1(layer3_0_relu_1)\n",
      "    layer3_1_bn1 = getattr(self.layer3, \"1\").bn1(layer3_1_conv1);  layer3_1_conv1 = None\n",
      "    layer3_1_relu = getattr(self.layer3, \"1\").relu(layer3_1_bn1);  layer3_1_bn1 = None\n",
      "    layer3_1_conv2 = getattr(self.layer3, \"1\").conv2(layer3_1_relu);  layer3_1_relu = None\n",
      "    layer3_1_bn2 = getattr(self.layer3, \"1\").bn2(layer3_1_conv2);  layer3_1_conv2 = None\n",
      "    add_5 = layer3_1_bn2 + layer3_0_relu_1;  layer3_1_bn2 = layer3_0_relu_1 = None\n",
      "    layer3_1_relu_1 = getattr(self.layer3, \"1\").relu(add_5);  add_5 = None\n",
      "    return {'feat1': layer1_1_relu_1, 'feat2': layer3_1_relu_1}\n",
      "    \n",
      "# To see more debug info, please use `graph_module.print_readable()`\n",
      "[('feat1', torch.Size([1, 64, 56, 56])), ('feat2', torch.Size([1, 256, 14, 14]))]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('feat1', torch.Size([1, 64, 56, 56])),\n",
       " ('feat2', torch.Size([1, 256, 14, 14]))]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Feature extraction with resnet\n",
    "model = torchvision.models.resnet18()\n",
    "tmr = get_graph_node_names(model)[0]\n",
    "print(tmr)\n",
    "# extract layer1 and layer3, giving as names `feat1` and feat2`\n",
    "model = create_feature_extractor(model, {'layer1': 'feat1', 'layer3': 'feat2'})\n",
    "print(model)\n",
    "out = model(torch.rand(1, 3, 224, 224))\n",
    "print([(k, v.shape) for k, v in out.items()])\n",
    "[('feat1', torch.Size([1, 64, 56, 56])), ('feat2', torch.Size([1, 256, 14, 14]))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Example model\n",
    "class ExampleModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ExampleModel, self).__init__()\n",
    "        self.conv = nn.Conv2d(3, 3, kernel_size=3, padding=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "# Initialize the model and optimizer\n",
    "model = ExampleModel()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Define the target and the input image as a learnable parameter\n",
    "target = torch.rand(1, 3, 32, 32)  # Example target\n",
    "input_image = torch.rand(1, 3, 32, 32, requires_grad=True)\n",
    "\n",
    "# Loop for iterative updates\n",
    "for i in range(num_iterations):\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Forward pass with the current input image\n",
    "    output = model(input_image)\n",
    "    \n",
    "    # Define a loss (e.g., MSE between output and target)\n",
    "    loss = nn.MSELoss()(output, target)\n",
    "    \n",
    "    # Backpropagation and image update\n",
    "    loss.backward()\n",
    "    \n",
    "    # Apply gradient update to input image\n",
    "    with torch.no_grad():\n",
    "        input_image -= input_image.grad * learning_rate\n",
    "        input_image.grad.zero_()  # Reset the gradient for the next iteration\n",
    "\n",
    "    # Optionally, perform weight updates\n",
    "    optimizer.step()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
